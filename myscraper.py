import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import os
import time
import logging
import random
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from concurrent.futures import ThreadPoolExecutor

# ==========================================
# 1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© (TITANIUM MAX)
# ==========================================

BASE_FILE = "users_database.csv"
DB_FILE = "rassim_titanium_max_2026.db" # Ø§Ù„Ø±Ø¨Ø· Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
BACKUP_FOLDER = "backups"

# Ù‚Ø§Ø¦Ù…Ø© User-Agents Ù…Ø­Ø¯Ø«Ø© Ù„Ø¹Ø§Ù… 2026 ØªØ´Ù…Ù„ Ø£Ø¬Ù‡Ø²Ø© Ø­Ø¯ÙŠØ«Ø©
HEADERS_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148",
    "Mozilla/5.0 (Linux; Android 14; Pixel 8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
]

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | [%(threadName)s] %(message)s"
)

# ==========================================
# 2ï¸âƒ£ Session Ø°ÙƒÙŠ Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø­Ù…Ø§ÙŠØ© (PRO PROXY-READY)
# ==========================================

def create_session():
    session = requests.Session()
    retries = Retry(
        total=7, # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        backoff_factor=1.5,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

# ==========================================
# 3ï¸âƒ£ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
# ==========================================

def clean_phone(phone):
    cleaned = re.sub(r"\D", "", str(phone))
    return cleaned[-9:] if len(cleaned) >= 9 else cleaned

def create_backup():
    if not os.path.exists(BASE_FILE): return
    if not os.path.exists(BACKUP_FOLDER): os.makedirs(BACKUP_FOLDER)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(BACKUP_FOLDER, f"backup_{timestamp}.csv")
    pd.read_csv(BASE_FILE).to_csv(backup_path, index=False)
    logging.info(f"ğŸ“¦ ØªÙ… ØªØ£Ù…ÙŠÙ† Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")

# ==========================================
# 4ï¸âƒ£ Ù…Ø­Ø±Ùƒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
# ==========================================

def process_source(source):
    """Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…ØµØ¯Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ (Ø¯Ø¹Ù… Multithreading)"""
    session = create_session()
    headers = {"User-Agent": random.choice(HEADERS_LIST), "Referer": "https://www.google.com/"}
    
    try:
        logging.info(f"ğŸ” ÙØ­Øµ Ø§Ù„Ù…Ù†ØµØ©: {source['platform']}...")
        # Ù‡Ù†Ø§ ÙŠØªÙ… ÙˆØ¶Ø¹ ÙƒÙˆØ¯ BeautifulSoup Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
        # Ø³Ù†Ø¨Ù‚ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ù‡ÙŠÙƒÙ„Ù‡Ø§
        
        simulated_data = [
            ["iPhone 15 Pro Max", 195000, "0550112233", "16-Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±", "Titanium Natural - Ø¬Ù„Ø¨ Ø¢Ù„ÙŠ"],
            ["Samsung S24 Ultra", 210000, "0661445566", "31-ÙˆÙ‡Ø±Ø§Ù†", "256GB - AI Features"],
            ["Google Pixel 8 Pro", 145000, "0770889900", "06-Ø¨Ø¬Ø§ÙŠØ©", "Ù…Ù…ØªØ§Ø² ÙƒØ£Ù†Ù‡ Ø¬Ø¯ÙŠØ¯"],
            ["Xiaomi 14 Ultra", 168000, "0555223344", "25-Ù‚Ø³Ù†Ø·ÙŠÙ†Ø©", "Global Version"],
            ["Oppo Reno 11", 72000, "0666778899", "42-ØªÙŠØ¨Ø§Ø²Ø©", "Ù‡Ù…Ø²Ø© Ø³ÙˆÙ‚ Ø§Ù„Ù†Ø®Ø¨Ø©"]
        ]
        
        results = []
        for item in simulated_data:
            item[2] = clean_phone(item[2])
            results.append(item)
            
        time.sleep(random.uniform(2, 4)) # ØªØ£Ø®ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø°ÙƒÙŠ
        return results
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {source['platform']}: {e}")
        return []

# ==========================================
# 5ï¸âƒ£ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª TITANIUM MAX
# ==========================================

def sync_to_sqlite(df):
    """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ±Ù‡Ø§ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        for _, row in df.iterrows():
            # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª (ads) Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
            cursor.execute("""
                INSERT INTO ads (product, price, phone, wilaya, description, date, owner)
                SELECT ?, ?, ?, ?, ?, ?, 'Auto-Bot'
                WHERE NOT EXISTS (
                    SELECT 1 FROM ads WHERE product=? AND price=? AND phone=?
                )
            """, (row['Product'], row['Price'], row['Phone'], row['City'], row['Description'], 
                  datetime.now().strftime("%Y-%m-%d"), row['Product'], row['Price'], row['Phone']))
            
        conn.commit()
        conn.close()
        logging.info("ğŸ—„ï¸ ØªÙ…Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQL Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø²Ø§Ù…Ù†Ø© SQL: {e}")

# ==========================================
# 6ï¸âƒ£ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Multithreaded Orchestrator)
# ==========================================

def run_titanium_scraper():
    logging.info("ğŸš€ ØªØ´ØºÙŠÙ„ Ù…Ø­Ø±Ùƒ RASSIM DZ TITANIUM MAX [Intelligence Mode]")
    create_backup()

    sources = [
        {"url": "https://market-1.dz/phones", "platform": "Market_North"},
        {"url": "https://market-2.dz/phones", "platform": "Market_West"},
        {"url": "https://market-3.dz/phones", "platform": "Market_East"},
    ]

    all_data = []
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ThreadPoolExecutor Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø¨Ø±Ù‚ ÙÙŠ Ø¬Ù„Ø¨ Ø¹Ø¯Ø© Ù…ØµØ§Ø¯Ø± Ù…Ø¹Ø§Ù‹
    with ThreadPoolExecutor(max_workers=3, thread_name_prefix="Scraper") as executor:
        future_results = executor.map(process_source, sources)
        for result in future_results:
            all_data.extend(result)

    if not all_data:
        logging.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©!")
        return

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù€ Pandas
    df_new = pd.DataFrame(all_data, columns=["Product", "Price", "Phone", "City", "Description"])
    df_new["Last_Update"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±)
    if os.path.exists(BASE_FILE):
        df_old = pd.read_csv(BASE_FILE)
        df_final = pd.concat([df_old, df_new]).drop_duplicates(subset=["Product", "Price", "Phone"], keep='last')
    else:
        df_final = df_new

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    df_final.sort_values(by="Price", ascending=True, inplace=True)
    df_final.to_csv(BASE_FILE, index=False)
    
    # Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙÙˆØ±ÙŠ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙŠØªØ§Ù†ÙŠÙˆÙ…
    sync_to_sqlite(df_new)

    logging.info(f"âœ… Ø§Ù„Ù…Ù‡Ù…Ø© ØªÙ…Øª Ø¨Ù†Ø¬Ø§Ø­ | Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³ÙˆÙ‚: {len(df_final)} Ø¥Ø¹Ù„Ø§Ù†")

if __name__ == "__main__":
    start = time.time()
    run_titanium_scraper()
    logging.info(f"â± Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {round(time.time() - start, 2)} Ø«Ø§Ù†ÙŠØ©")
